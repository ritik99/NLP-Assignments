{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from urllib import request\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sklearn\n",
    "url = 'http://www.gutenberg.org/cache/epub/1661/pg1661.txt'\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This section removes punctuation and miscellaneous text. It also adds the sentence start and ending tag. List for the short words obtained from: https://stackoverflow.com/questions/43018030/replace-appostrophe-short-words-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = ['Project Gutenberg\\'s The Adventures of Sherlock Holmes, by Arthur Conan Doyle',\n",
    "       'This eBook is for the use of anyone anywhere at no cost and with',\n",
    "       'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
    "       're-use it under the terms of the Project Gutenberg License included',\n",
    "       'with this eBook or online at www.gutenberg.net',\n",
    "       'Title: The Adventures of Sherlock Holmes',\n",
    "       'Author: Arthur Conan Doyle',\n",
    "       'Posting Date: April 18, 2011 [EBook #1661]',\n",
    "       'First Posted: November 29, 2002', 'Language: English',\n",
    "        '*** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***',\n",
    "        'Produced by an anonymous Project Gutenberg volunteer and Jose Menendez', 'THE ADVENTURES OF SHERLOCK HOLMES',\n",
    "        'by', 'SIR ARTHUR CONAN DOYLE', 'I. A Scandal in Bohemia',\n",
    "        'II. The Red-headed League', 'III. A Case of Identity', 'IV. The Boscombe Valley Mystery', 'V. The Five Orange Pips',\n",
    "        'VI. The Man with the Twisted Lip', 'VII. The Adventure of the Blue Carbuncle', 'VIII. The Adventure of the Speckled Band',\n",
    "        'IX. The Adventure of the Engineer\\'s Thumb', 'X. The Adventure of the Noble Bachelor', 'XI. The Adventure of the Beryl Coronet',\n",
    "        'XII. The Adventure of the Copper Beeches', ',',';',':','!','\\\"','\\'','-']\n",
    "\n",
    "for i in replace:\n",
    "    raw.replace(i, '')\n",
    "    \n",
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he shall have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I shall have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it shall have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall\",\n",
    "\"she'll've\": \"she shall have\",\n",
    "\"she's\": \"she has\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall\",\n",
    "\"they'll've\": \"they shall have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall\",\n",
    "\"what'll've\": \"what shall have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall\",\n",
    "\"who'll've\": \"who shall have\",\n",
    "\"who's\": \"who has\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall\",\n",
    "\"you'll've\": \"you shall have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "for key in contractions.keys():\n",
    "    raw.replace(key, contractions[key])\n",
    "    \n",
    "t = nltk.sent_tokenize(raw)\n",
    "\n",
    "tt = []\n",
    "size = 0\n",
    "for line in t:\n",
    "    temp = line.replace('\\r', ' ')\n",
    "    temp = temp.replace('\\n', ' ')\n",
    "    temp = '<s> ' + temp\n",
    "    temp = temp.replace('.', ' </s>')\n",
    "    temp = temp.replace('?', ' </s>')\n",
    "    size += len(line.split())\n",
    "    temp = temp.lower()\n",
    "    tt.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part makes the train and test parts in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = sklearn.model_selection.train_test_split(tt, train_size = 0.8, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting frequencies of n-grams\n",
    "This part calculates the frequencies of unigram, bigram and trigrams present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count = {}      #stores the unigram model\n",
    "bigram_count = {}       #stores the bigram model\n",
    "trigram_count = {}      #stores the trigram model\n",
    "quadgram_count = {}     #stores the quadgram model\n",
    "\n",
    "token_count = 0\n",
    "word_count = {}         #single word count\n",
    "two_word_count = {}     #double word count\n",
    "three_word_count = {}   #three word count\n",
    "four_word_count = {}    #four word count\n",
    "two_word_count_generate = {}\n",
    "total_word_count = 0\n",
    "\n",
    "\n",
    "for line in train:\n",
    "    for word in line.split():\n",
    "        try:\n",
    "            word_count[word] += 1\n",
    "        except:\n",
    "            word_count[word] = 1\n",
    "\n",
    "            \n",
    "types = list(word_count.keys())\n",
    "\n",
    "for i in range(len(types)):\n",
    "    for j in range(i+1, len(types)):\n",
    "        two_word_count[(types[i], types[j])] = 0\n",
    "        two_word_count[(types[j], types[i])] = 0\n",
    "for i in word_count.keys():\n",
    "    total_word_count += word_count[i]\n",
    "    \n",
    "    \n",
    "for line in train:\n",
    "    t = line.split()\n",
    "    for i in range(1, len(t)):\n",
    "        temp = (t[i-1], t[i])\n",
    "        try:\n",
    "            two_word_count_generate[temp] += 1\n",
    "            two_word_count[temp] += 1\n",
    "        except:\n",
    "            two_word_count_generate[temp] = 1\n",
    "            two_word_count[temp] = 1\n",
    "\n",
    "\n",
    "for line in train:\n",
    "    t = line.split()\n",
    "    for i in range(2, len(t)):\n",
    "        temp = (t[i-2], t[i-1], t[i])\n",
    "        try:\n",
    "            three_word_count[temp] += 1\n",
    "        except:\n",
    "            three_word_count[temp] = 1\n",
    "\n",
    "\n",
    "for line in train:\n",
    "    t = line.split()\n",
    "    for i in range(3, len(t)):\n",
    "        temp = (t[i-3], t[i-2], t[i-1], t[i])\n",
    "        try:\n",
    "            four_word_count[temp] += 1\n",
    "        except:\n",
    "            four_word_count[temp] = 1\n",
    "\n",
    "\n",
    "\n",
    "for word in word_count.keys():\n",
    "    token_count += word_count[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building MLE for n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram():\n",
    "    for line in train:\n",
    "        temp = line.split()\n",
    "        for i in range(0, len(temp)):\n",
    "            temp1 = temp[i]\n",
    "            if temp1 not in unigram_count.keys():\n",
    "                unigram_count[temp1] = word_count[temp1]/total_word_count\n",
    "def bigram():\n",
    "    for line in train:\n",
    "        temp = line.split()\n",
    "        for i in range(1, len(temp)):\n",
    "            temp1 = (temp[i-1], temp[i])\n",
    "            if temp1 not in bigram_count.keys():\n",
    "                bigram_count[temp1] = two_word_count[temp1]/word_count[temp[i-1]]\n",
    "                \n",
    "                \n",
    "def trigram():\n",
    "    for line in train:\n",
    "        temp = line.split()\n",
    "        for i in range(2, len(temp)):\n",
    "            temp1 = (temp[i-2], temp[i-1], temp[i])\n",
    "            if temp1 not in trigram_count.keys():\n",
    "                trigram_count[temp1] = three_word_count[temp1]/two_word_count[(temp[i-2], temp[i-1])]\n",
    "\n",
    "def quadgram():\n",
    "    for line in train:\n",
    "        temp = line.split()\n",
    "        for i in range(3, len(temp)):\n",
    "            temp1 = (temp[i-3], temp[i-2], temp[i-1], temp[i])\n",
    "            if temp1 not in quadgram_count.keys():\n",
    "                quadgram_count[temp1] = four_word_count[temp1]/three_word_count[(temp[i-3], temp[i-2], temp[i-1])]           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram()\n",
    "bigram()\n",
    "trigram()\n",
    "quadgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generation using n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model):\n",
    "    min_len = 8\n",
    "    max_len = 15\n",
    "    sent = []\n",
    "    if model == \"unigram\":\n",
    "        sent_length = 0\n",
    "        while(sent_length < 15):\n",
    "            temp = np.random.choice(list(unigram_count.keys()), 1, list(unigram_count.values()))[0]\n",
    "            temp = str(temp)\n",
    "            if temp == \"</s>\":\n",
    "                if len(sent) >= 8:\n",
    "                    sent.append(temp)\n",
    "                    sent_length += 1\n",
    "                    break\n",
    "            else:\n",
    "                sent_length += 1\n",
    "                sent.append(temp)\n",
    "        if sent_length == 15 and sent[14] != \"</s>\":\n",
    "            sent.append(\"</s>\")\n",
    "    \n",
    "    elif model == \"bigram\":\n",
    "        sent.append(\"<s>\")\n",
    "        sent_length = 1\n",
    "        attempts = 0\n",
    "        while sent_length < 15:\n",
    "            possible_words= []\n",
    "            possible_probabilities = []\n",
    "            for bigram in bigram_count.keys():\n",
    "                if bigram[0] == sent[sent_length - 1]:\n",
    "                    possible_words.append(bigram[1])\n",
    "                    possible_probabilities.append(bigram_count[bigram])\n",
    "            if len(possible_words) == 0:\n",
    "                sentence = generate(\"bigram\")\n",
    "                return sentence\n",
    "            temp = np.random.choice(possible_words, 1, possible_probabilities)[0]\n",
    "            temp = str(temp)\n",
    "            if temp == \"</s>\":\n",
    "                attempts += 1\n",
    "                if len(sent) >= 8:\n",
    "                    sent.append(temp)\n",
    "                    sent_length += 1\n",
    "                    break\n",
    "                elif attempts == 20:\n",
    "                    sent.append(temp)\n",
    "                    break\n",
    "            else:\n",
    "                attempts = 0\n",
    "                sent_length += 1\n",
    "                sent.append(temp)\n",
    "        if sent_length == 15 and sent[14] != \"</s>\":\n",
    "            sent.append(\"</s>\")\n",
    "            \n",
    "    elif model == \"trigram\":\n",
    "        possible_start = []\n",
    "        possible_probabilities = []\n",
    "        denominator = 0\n",
    "        for val in two_word_count_generate.values():\n",
    "            denominator += val\n",
    "        for temp in two_word_count_generate.keys():\n",
    "            if temp[0] == \"<s>\":\n",
    "                possible_start.append(temp)\n",
    "                possible_probabilities.append(two_word_count_generate[temp]/denominator)\n",
    "        start = np.random.choice(list(range(len(possible_start))), 1, possible_probabilities)[0]\n",
    "        sent.append(possible_start[start][0])\n",
    "        sent.append(possible_start[start][1])\n",
    "        sent_length = 2\n",
    "        #print(sent)\n",
    "        attempts = 0\n",
    "        while sent_length < 15:\n",
    "            #print(sent)\n",
    "            possible_words= []\n",
    "            possible_probabilities = []\n",
    "            for trigram in trigram_count.keys():\n",
    "                if trigram[0] == sent[sent_length - 2] and trigram[1] == sent[sent_length - 1]:\n",
    "                    #print(\"HERE\")\n",
    "                    possible_words.append(trigram[2])\n",
    "                    possible_probabilities.append(trigram_count[trigram])\n",
    "            if len(possible_words) == 0:\n",
    "                sentence = generate(\"trigram\")\n",
    "                return sentence\n",
    "            temp = np.random.choice(possible_words, 1, possible_probabilities)[0]\n",
    "            temp = str(temp)\n",
    "            if temp == \"</s>\":\n",
    "                attempts += 1\n",
    "                if len(sent) >= 8:\n",
    "                    sent.append(temp)\n",
    "                    sent_length += 1\n",
    "                    break\n",
    "                elif attempts == 20:\n",
    "                    sent.append(temp)\n",
    "                    break\n",
    "            else:\n",
    "                attempts = 0\n",
    "                sent_length += 1\n",
    "                sent.append(temp)\n",
    "        if sent_length == 15 and sent[14] != \"</s>\":\n",
    "            sent.append(\"</s>\")\n",
    "        \n",
    "    elif model == \"quadgram\":\n",
    "        possible_start = []\n",
    "        possible_probabilities = []\n",
    "        denominator = 0\n",
    "        for val in three_word_count.values():\n",
    "            denominator += val\n",
    "        for temp in three_word_count.keys():\n",
    "            if temp[0] == \"<s>\":\n",
    "                possible_start.append(temp)\n",
    "                possible_probabilities.append(three_word_count[temp]/denominator)\n",
    "        start = np.random.choice(list(range(len(possible_start))), 1, possible_probabilities)[0]\n",
    "        sent.append(possible_start[start][0])\n",
    "        sent.append(possible_start[start][1])\n",
    "        sent.append(possible_start[start][2])\n",
    "        sent_length = 3\n",
    "        #print(sent)\n",
    "        attempts = 0\n",
    "        while sent_length < 15:\n",
    "            #print(sent)\n",
    "            possible_words= []\n",
    "            possible_probabilities = []\n",
    "            for quadgram in quadgram_count.keys():\n",
    "                if quadgram[0] == sent[sent_length - 3] and quadgram[1] == sent[sent_length - 2] and quadgram[2] == sent[sent_length - 1]:\n",
    "                    #print(\"HERE\")\n",
    "                    possible_words.append(quadgram[3])\n",
    "                    possible_probabilities.append(quadgram_count[quadgram])\n",
    "            if len(possible_words) == 0:\n",
    "                sentence = generate(\"bigram\")\n",
    "                return sentence\n",
    "            temp = np.random.choice(possible_words, 1, possible_probabilities)[0]\n",
    "            temp = str(temp)\n",
    "            if temp == \"</s>\":\n",
    "                attempts += 1\n",
    "                if len(sent) >= 8:\n",
    "                    sent.append(temp)\n",
    "                    sent_length += 1\n",
    "                    break\n",
    "                elif attempts == 20:\n",
    "                    sent.append(temp)\n",
    "                    break\n",
    "            else:\n",
    "                attempts = 0\n",
    "                sent_length += 1\n",
    "                sent.append(temp)\n",
    "        if sent_length == 15 and sent[14] != \"</s>\":\n",
    "            sent.append(\"</s>\")\n",
    "                    \n",
    "        \n",
    "    \n",
    "    sentence = sent[0]\n",
    "    for i in range(1, len(sent)):\n",
    "        sentence = sentence + \" \" + sent[i]\n",
    "    return sentence\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the example sentences being generated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lap nursery deceased evolve upward condescend 'neither rose steps--for degenerating mission knee, white pays was--'and.\n",
      "time holmes' \"now transition save, village managed hansom, cheerful feasible \"'ah, horror, assizes, calamity rending,.\n",
      "puzzled drink returned, engine nonsense stamp smile clean-cut, arranged,' complimentary fowler act, deadly suburban bars.\n",
      "\"there waned \"we've 'for knees, mice, china, lime-cream closed eavesdroppers metal, stepfather's burden curse proprietary.\n",
      "children fare haste \"oh, adventures staggered, gently arthur's arrived correspondent, pound \"dear purveyor.h utilise.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    t = generate(\"unigram\")\n",
    "    t = t.replace('<s>', '')\n",
    "    t = t.replace(' </s>', '.')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anyhow, he answered; \"i begin to lose.\n",
      " folk who will put so vile alley lurking behind.\n",
      " \"in that colonel ushered in it is, however, which made inquiries are then roused.\n",
      " volunteers with damp and john swain, of uffa, and colleague, dr.\n",
      " some friend was late in at coburg square, and penetrating grey dressing-gown, a letter.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    t = generate(\"bigram\")\n",
    "    t = t.replace('<s>', '')\n",
    "    t = t.replace(' </s>', '.')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"we're hunting in couples again, doctor, you see,\" said i ruefully, pointing to a.\n",
      " \"terse and to come with me, and it had cleared it all over, with.\n",
      " recently he has agreed to donate royalties under this one twelve months i find.\n",
      " \"that sounds a little green-scummed pool, which is really managed by miss mary sutherland,.\n",
      " during all the terms of use and redistributing project gutenberg-tm trademark as set forth.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    t = generate(\"trigram\")\n",
    "    t = t.replace('<s>', '')\n",
    "    t = t.replace(' </s>', '.')\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"well then, i shan't tell you.\n",
      " \"'this is mr.\n",
      " here he comes.\n",
      " if horner were in danger it would be the blackest treachery to holmes to.\n",
      " he cried, throwing his fat hands out into the street.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    t = generate(\"quadgram\")\n",
    "    t = t.replace('<s>', '')\n",
    "    t = t.replace(' </s>', '.')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sentence, model):\n",
    "    if model == \"unigram\":\n",
    "        temp = sentence.split()\n",
    "        ans = 0\n",
    "        for word in temp:\n",
    "            try:\n",
    "                ans += np.log(unigram_count[word])\n",
    "            except:\n",
    "                return 0\n",
    "    elif model == \"bigram\":\n",
    "        temp = sentence.split()\n",
    "        ans = 0\n",
    "        for i in range(1, len(temp)):\n",
    "            try:\n",
    "                ans += np.log(bigram_count[(temp[i-1], temp[i])])\n",
    "            except:\n",
    "                return 0\n",
    "    \n",
    "    elif model == \"trigram\":\n",
    "        temp = sentence.split()\n",
    "        ans = 0\n",
    "        for i in range(2, len(temp)):\n",
    "            try:\n",
    "                ans += np.log(trigram_count[(temp[i-2], temp[i-1], temp[i])])\n",
    "            except:\n",
    "                return 0\n",
    "    elif model == \"quadgram\":\n",
    "        temp = sentence.split()\n",
    "        ans = 0\n",
    "        for i in range(3, len(temp)):\n",
    "            try:\n",
    "                ans += np.log(quadgram_count[(temp[i-3], temp[i-2], temp[i-1], temp[i])])\n",
    "            except:\n",
    "                return 0\n",
    "    return ans\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how the function works. The first input is the sentence and the second input is the model to be used to find the probability. The output is in log-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-130.22575942365347\n",
      "-62.75278951165759\n",
      "-17.46298516544099\n",
      "-5.886104031450156\n"
     ]
    }
   ],
   "source": [
    "print(probability(\"i could see by his manner that he had stronger reasons for  satisfaction than his words alone would imply\", \"unigram\"))\n",
    "print(probability(\"i could see by his manner that he had stronger reasons for  satisfaction than his words alone would imply\", \"bigram\"))\n",
    "print(probability(\"i could see by his manner that he had stronger reasons for  satisfaction than his words alone would imply\", \"trigram\"))\n",
    "print(probability(\"i could see by his manner that he had stronger reasons for  satisfaction than his words alone would imply\", \"quadgram\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bigram_model = {}\n",
    "for i in range(len(types)):\n",
    "    for j in range(i, len(types)):\n",
    "        try:\n",
    "            new_bigram_model[(types[i], types[j])] = (two_word_count[(types[i], types[j])] + 1)*word_count[types[i]]/(word_count[types[i]] + len(types))\n",
    "        except:\n",
    "            new_bigram_model[(types[i], types[j])] = (1)*word_count[types[i]]/(word_count[types[i]] + len(types))\n",
    "\n",
    "for i in range(len(types)):\n",
    "    for j in range(i, len(types)):\n",
    "        try:\n",
    "            new_bigram_model[(types[j], types[i])] = (two_word_count[(types[j], types[i])] + 1)*word_count[types[j]]/(word_count[types[j]] + len(types))\n",
    "        except:\n",
    "            new_bigram_model[(types[j], types[i])] = 1*word_count[types[j]]/(word_count[types[j]] + len(types))\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "del two_word_count_generate\n",
    "highest_d = 0\n",
    "second_d = 0\n",
    "for key in two_word_count.keys():\n",
    "    if abs(two_word_count[key] - new_bigram_model[key]) > highest_d:\n",
    "        second_d = highest_d\n",
    "        highest_d = abs(two_word_count[key] - new_bigram_model[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in count values\n",
    "The largest differences in counts are listed below. This difference is resulting because earlier some bigrams had a count of 0 while some had a very high word count. Smoothing transfers the counts from those that have very counts to those that have very low counts. This sometimes leads to considerable changes in the value of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(highest_d)\n",
    "print(second_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Turing Smoothing\n",
    "The next few blocks implement the Good Turing Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_bigram_model = {}\n",
    "freq_ofreq = {}\n",
    "for freq in two_word_count.values():\n",
    "    try:\n",
    "        freq_ofreq[freq] += 1\n",
    "    except:\n",
    "        freq_ofreq[freq] = 1\n",
    "        \n",
    "max_bigram_count = len(types)*len(types)\n",
    "\n",
    "new_freq_ofreq = {}\n",
    "for freq in range(0, 11):\n",
    "    new_freq_ofreq[freq] = (freq_ofreq[freq+1]*(freq+1))/freq_ofreq[freq]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity values\n",
    "The following block calculates the perplexity values. The perplexity was calculated using the method described here: https://stats.stackexchange.com/questions/129352/how-to-find-the-perplexity-of-a-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Turing Perplexity:  729.2924606834878\n",
      "Add-one Perplexity:  2293.251455408744\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "for i in range(1, 11):\n",
    "    temp += i - new_freq_ofreq[i]\n",
    "D = temp/10\n",
    "for freq in freq_ofreq.keys():\n",
    "    if freq not in new_freq_ofreq.keys():\n",
    "        new_freq_ofreq[freq] = freq - D\n",
    "\n",
    "        \n",
    "s_gt = 0\n",
    "num_words = 0\n",
    "s_ao = 0\n",
    "for line in test:\n",
    "    temp = line.split()\n",
    "    num_words += len(temp)\n",
    "    temp1 = 0\n",
    "    temp5 = 0\n",
    "    for i in range(1, len(temp)):\n",
    "        try:\n",
    "            temp3 = two_word_count[(temp[i-1], temp[i])]\n",
    "            temp4 = new_bigram_model[(temp[i-1], temp[i])]\n",
    "        except:\n",
    "            temp3 = 0\n",
    "            temp4 = 1/len(types)\n",
    "        two_count = new_freq_ofreq[temp3]\n",
    "        #print(\"GT \", two_count)\n",
    "        #print(\"AO \", temp4)\n",
    "        try:\n",
    "            temp1 += np.log(two_count/word_count[temp[i-1]])\n",
    "            temp5 += np.log(temp4/word_count[temp[i-1]])\n",
    "        except:\n",
    "            temp1 += np.log(two_count)\n",
    "            temp5 += np.log(temp4)\n",
    "    s_gt += temp1\n",
    "    s_ao += temp5\n",
    "\n",
    "print(\"Good Turing Perplexity: \", np.exp(-1*(s_gt/num_words)))\n",
    "print(\"Add-one Perplexity: \", np.exp(-1*(s_ao/num_words)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of n-grams in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams that exist:  47091  Max. possible bigrams:  112593321\n",
      "Number of trigrams that exist:  72790  Max. possible trigrams:  1194727729131\n",
      "Number of quadgrams that exist:  77137  Max. possible quadgrams:  12677255933809041\n"
     ]
    }
   ],
   "source": [
    "num_bi = 0\n",
    "for i in two_word_count.values():\n",
    "    if i != 0:\n",
    "        num_bi += 1\n",
    "\n",
    "num_tri = 0\n",
    "for i in three_word_count.values():\n",
    "    if i != 0:\n",
    "        num_tri += 1\n",
    "    \n",
    "num_quad = 0\n",
    "for i in four_word_count.values():\n",
    "    if i != 0:\n",
    "        num_quad += 1\n",
    "    \n",
    "print(\"Number of bigrams that exist: \", num_bi, \" Max. possible bigrams: \", len(types)*len(types))\n",
    "print(\"Number of trigrams that exist: \", num_tri, \" Max. possible trigrams: \", len(types)*len(types)*len(types))\n",
    "print(\"Number of quadgrams that exist: \", num_quad, \" Max. possible quadgrams: \", len(types)*len(types)*len(types)*len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732893919338718\n",
      "0.8610436893203883\n",
      "1.1289291422482686\n",
      "0.9020501138952164\n",
      "0.9522058823529411\n",
      "1.4414168937329697\n",
      "1.1087866108786608\n",
      "0.8409090909090908\n",
      "1.1428571428571432\n",
      "0.5999999999999996\n",
      "Average value of d:  0.9711092485533397\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "for i in range(1, 11):\n",
    "    print(i - new_freq_ofreq[i])\n",
    "    temp += i - new_freq_ofreq[i]\n",
    "    \n",
    "print(\"Average value of d: \", temp/10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
